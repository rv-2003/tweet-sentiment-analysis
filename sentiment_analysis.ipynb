{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCuDEw2HuIDm"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSOAOPKcpwcX",
    "outputId": "bb5cd03e-d882-43bc-c8b9-408121298f43"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Basic libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Preprocessing libraries\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For Model training\n",
    "from scipy.stats import uniform                 # Used to sample hyperparameter values from a continuous range.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC              # a variant of SVC optimized for large datasets\n",
    "\n",
    "# Metrics for accuracy\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZZ16S-Vh7Wr",
    "outputId": "0d3fa1f1-7078-4154-bcf5-33f32d690bb6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Accessing the dataset\n",
    "dataset_path = \"/content/drive/MyDrive/tweet/tweet.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvKtvC0vuDx4"
   },
   "source": [
    "# Reading our Dataset\n",
    "## Dataset details\n",
    "- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- ids: The id of the tweet ( 2087)\n",
    "- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (robotickilldozr)\n",
    "- text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "OKLaQjV_pMPA",
    "outputId": "fa104b06-0fc9-4329-d7c2-71ffc291e178"
   },
   "outputs": [],
   "source": [
    "columns=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df_read=pd.read_csv(dataset_path,encoding='latin1',names=columns)\n",
    "print(df_read.shape)\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SV9kG6lGuV7a"
   },
   "source": [
    "### Making a DataFrame out of the above Dataset with the only columns that are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Kv4tOiMruP0b",
    "outputId": "fb2b9b91-75dc-4c7d-e5d4-3ba4d10240a6"
   },
   "outputs": [],
   "source": [
    "data={'text':df_read['text'].values,'target':df_read['target'].values}\n",
    "df=pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "ePWbwQY4ujPQ",
    "outputId": "c449f1ee-0951-4065-c26b-175688d78324"
   },
   "outputs": [],
   "source": [
    "# Seeing the distribution of positive and negative tweet reviews in target column\n",
    "plt.figure(figsize=(7,3))\n",
    "sns.countplot(data=df,x='target',palette=['green','red'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml1gJTrSujRh"
   },
   "outputs": [],
   "source": [
    "# Data clearing and preprocessing\n",
    "corpus = []\n",
    "ps=PorterStemmer()\n",
    "for i in range(len(df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])         # Removing special characters from text(message)\n",
    "    review = review.lower()                                  # Converting entire text into lower case\n",
    "    review = review.split()                                  # Splitting our text into words\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]             # Stemming and removing stopwords\n",
    "    review = ' '.join(review)                                # Joining all the words into a comple text\n",
    "    corpus.append(review)                                    # Appending each text into the list corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEPejMzSkH9q"
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "cv = TfidfVectorizer(ngram_range=(1,2), max_features=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvCFnQI1z54K"
   },
   "outputs": [],
   "source": [
    "# We will use X as independent feature section\n",
    "X = cv.fit_transform(corpus)\n",
    "# We will use y as dependent feature section\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7EAxeONuGXc",
    "outputId": "2dab0e62-9bd4-4000-9d91-a290d4509d85"
   },
   "outputs": [],
   "source": [
    "print('No. of feature_words: ', len(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8O_JnTBRwmY"
   },
   "outputs": [],
   "source": [
    "# Creating a pickle file for the TfidfVectorizer\n",
    "with open('cv-transform.pkl', 'wb') as f:\n",
    "    pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhXMfa1qLLhu"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkseYTm31SBd"
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0POUSA4Rw4sX"
   },
   "outputs": [],
   "source": [
    "model1=LogisticRegression()\n",
    "model2=BernoulliNB()\n",
    "model3=LinearSVC()\n",
    "model=[model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJfjM-zCxfvu",
    "outputId": "b0c4be6b-d0b7-4444-9b29-04b9a5d86c0a"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for algo in model:\n",
    "  i += 1\n",
    "  print(\"M-O-D-E-L :\",i)\n",
    "  algo.fit(X_train, y_train)\n",
    "  y_pred=algo.predict(X_test)\n",
    "  # Checking the accuracy\n",
    "  print(\"Confusion matrix : \\n\",confusion_matrix(y_pred,y_test))\n",
    "  print(\"Accuracy score : \",accuracy_score(y_pred,y_test))\n",
    "  print(\"Classification Report : \\n\",classification_report(y_pred,y_test))\n",
    "  print(\"-----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahf8QCxh5ZYi"
   },
   "source": [
    "NOTE :- Model1 is performing the best i.e. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeczTChsh1Qf"
   },
   "source": [
    "## Doing Hyperparameter Tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVvhl53DSGaj"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters to be tuned and their search ranges\n",
    "param_dist = {'C': uniform(0.1, 1.0),\n",
    "              'penalty': ['l2'],\n",
    "              'solver': ['liblinear', 'saga']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "fQtQhhX_huFS",
    "outputId": "a0ab6188-603b-447b-f5e8-56fb69732ae4"
   },
   "outputs": [],
   "source": [
    "LogisticRegression = RandomizedSearchCV(estimator=model1,param_distributions=param_dist,n_iter=10, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "LogisticRegression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfF9LiY1huJI",
    "outputId": "a32f2de3-57db-480e-c449-431cfe72ea0b"
   },
   "outputs": [],
   "source": [
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", LogisticRegression.best_params_)\n",
    "print(\"Best score: \", LogisticRegression.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXe6d_v0huNd"
   },
   "outputs": [],
   "source": [
    "# Training model using Naive bayes classifier\n",
    "y_pred=LogisticRegression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xavmsabMhuQv",
    "outputId": "c41cf3ea-a547-453e-c4da-060c2b1f6d66"
   },
   "outputs": [],
   "source": [
    "# Checking the accuracy\n",
    "print(\"Confusion matrix : \\n\",confusion_matrix(y_pred,y_test))\n",
    "print(\"Accuracy score : \",accuracy_score(y_pred,y_test))\n",
    "print(\"Classification Report : \\n\",classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kky1KWOlim6A"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating a pickle file for the Logistic Regression model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweetmodel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(LogisticRegression,file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating a pickle file for the Logistic Regression model\n",
    "with open(\"tweetmodel.pkl\",\"wb\") as file:\n",
    "  pickle.dump(LogisticRegression,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIhKTs61qRTg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
